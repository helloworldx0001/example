{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy is 0.0776 \n",
      "After 100 training step(s), validation accuracy is 0.9344 \n",
      "After 200 training step(s), validation accuracy is 0.9532 \n",
      "After 300 training step(s), validation accuracy is 0.9616 \n",
      "After 400 training step(s), validation accuracy is 0.9662 \n",
      "After 500 training step(s), validation accuracy is 0.9716 \n",
      "After 600 training step(s), validation accuracy is 0.9732 \n",
      "After 700 training step(s), validation accuracy is 0.9744 \n",
      "After 800 training step(s), validation accuracy is 0.977 \n",
      "After 900 training step(s), validation accuracy is 0.9782 \n",
      "After 1000 training step(s), validation accuracy is 0.98 \n",
      "After 1100 training step(s), validation accuracy is 0.979 \n",
      "After 1200 training step(s), validation accuracy is 0.981 \n",
      "After 1300 training step(s), validation accuracy is 0.983 \n",
      "After 1400 training step(s), validation accuracy is 0.9834 \n",
      "After 1500 training step(s), validation accuracy is 0.9838 \n",
      "After 1600 training step(s), validation accuracy is 0.984 \n",
      "After 1700 training step(s), validation accuracy is 0.9838 \n",
      "After 1800 training step(s), validation accuracy is 0.9846 \n",
      "After 1900 training step(s), validation accuracy is 0.985 \n",
      "After 2000 training step(s), validation accuracy is 0.9844 \n",
      "After 2100 training step(s), validation accuracy is 0.9842 \n",
      "After 2200 training step(s), validation accuracy is 0.9858 \n",
      "After 2300 training step(s), validation accuracy is 0.9856 \n",
      "After 2400 training step(s), validation accuracy is 0.987 \n",
      "After 2500 training step(s), validation accuracy is 0.9866 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# MNIST数据集相关的常数。\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# 配置神经网络的参数。\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 基础学习率以及学习率衰减指数\n",
    "LEARNING_RATE_BASE = 0.05\n",
    "LEARNING_RATE_DECAY = 0.99    \n",
    "\n",
    "KEEP_PROB = 0.5\n",
    "\n",
    "# 正则化权重\n",
    "REGULARAZTION_RATE = 0.001\n",
    "\n",
    "TRAINING_STEPS = 5000\n",
    "\n",
    "# 网络结构参数\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "FC_SIZE = 512\n",
    "\n",
    "# 定义神经网络结构\n",
    "def inference(input_tensor, train, reuse, regularizer):\n",
    "    with tf.variable_scope('layer1-conv1', reuse = reuse):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            \"weight\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "    with tf.variable_scope(\"layer3-conv2\", reuse = reuse):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            \"weight\", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "        reshaped = tf.reshape(pool2, [-1, nodes])\n",
    "\n",
    "    with tf.variable_scope('layer5-fc1', reuse = reuse):\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes, FC_SIZE],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    with tf.variable_scope('layer6-fc2', reuse = reuse):\n",
    "        fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit\n",
    "\n",
    "# TensorFlow计算图创建过程。\n",
    "def define_graph():\n",
    "    x = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    y = inference(x, True, False, regularizer)\n",
    "    y_test = inference(x, False, True, None)\n",
    "    \n",
    "    # 定义存储训练轮数的变量。 \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数。\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    loss = tf.reduce_mean(cross_entropy) + tf.add_n(tf.get_collection('losses'))\n",
    " \n",
    "    # 设置指数衰减的学习率。\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    # 使用tf.train.GradientDescentOptimizer优化算法来优化损失函数。\n",
    "    train_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 检验神经网络的正确率。\n",
    "    correct_prediction = tf.equal(tf.argmax(y_test, 1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return x, y_, train_op, accuracy\n",
    "\n",
    "# 训练模型的过程。\n",
    "def train(x, y_, train_op, accuracy, mnist):\n",
    "    # 初始化会话并开始训练过程。\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据。一般在神经网络的训练过程中会通过验证数据来大致判断停止的\n",
    "        # 条件和评判训练的效果。\n",
    "        validate_feed = {x: np.reshape(mnist.validation.images, \n",
    "                                       (-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)), \n",
    "                         y_: mnist.validation.labels}\n",
    "\n",
    "        # 准备测试数据。在真实的应用中，这部分数据在训练时是不可见的，这个数据只是作为  \n",
    "        # 模型优劣的最后评价标准。\n",
    "        test_feed = {x: np.reshape(mnist.test.images,\n",
    "                                   (-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)), \n",
    "                     y_: mnist.test.labels}\n",
    "\n",
    "        # 迭代地训练神经网络。\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证数据集上的测试结果。\n",
    "            if i % 100 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy is %g \" % (i, validate_acc))\n",
    "\n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程。\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshaped_xs = np.reshape(xs, (-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "            sess.run(train_op, feed_dict={x: reshaped_xs, y_: ys})\n",
    "            \n",
    "        # 在训练结束之后，在测试数据上检测神经网络模型的最终正确率。\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After %d training step(s), test accuracy is %g\" % (TRAINING_STEPS, test_acc))\n",
    "\n",
    "# 主程序入口\n",
    "if __name__ == '__main__':\n",
    "    # 声明处理MNIST数据集的类，这个类在初始化时会自动下载数据。\n",
    "    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n",
    "    x, y_, train_op, accuracy = define_graph()\n",
    "    train(x, y_, train_op, accuracy, mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
